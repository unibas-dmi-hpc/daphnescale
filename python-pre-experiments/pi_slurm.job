#!/bin/bash
#SBATCH --partition=cpu
#SBATCH --hint=nomultithread
#SBATCH --mem=64G

# ------------------------------
# Metadata & basic setup
# ------------------------------

rep=$SLURM_ARRAY_TASK_ID
# Identifier for output
if (( profile == 0 )); then
    ident=${ident_base}_r${rep}
else
    ident=${ident_base}_profile
fi

# Script to run
script="${path}pi.py"

# Singularity image
sif="${path}sif/${sing_img}/${sing_img}.sif"

# Arguments to Python script
cmd_args=("$script" "$kind" "$n")

# ------------------------------
# MPI mode (set SLURMâ€™s MPI interface)
# ------------------------------
if [[ $parallel_type == "mpi" ]]; then
    module purge
    module load OpenMPI/4.1.6-GCC-13.2.0

    # Pass host OpenMPI libs into the container
    export SINGULARITYENV_LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/opt/python314t/lib"

    # Tell mpi4py WHICH libmpi to dlopen
    MPI_LIB="/cvmfs/sling.si/modules/el7/software/OpenMPI/4.1.6-GCC-13.2.0/lib/libmpi.so.40.30.6"
    export SINGULARITYENV_MPI4PY_LIBMPI="$MPI_LIB"

    # Transport UCX/OMPI/PMIX hints into the container as well
    export UCX_TLS=self,sm
    export OMPI_MCA_PML=ucx
    export PMIX_MCA_gds=hash

    export SINGULARITYENV_UCX_TLS="$UCX_TLS"
    export SINGULARITYENV_OMPI_MCA_PML="$OMPI_MCA_PML"
    export SINGULARITYENV_PMIX_MCA_gds="$PMIX_MCA_gds"

    mpi_flag="--mpi=pmix"

    # Keep exec simple, like in the working test job
    cmd=("singularity" "exec" "-B" "/cvmfs:/cvmfs")

else
    mpi_flag="--mpi=none"
    cmd=("singularity" "exec")
fi

# ------------------------------
# Python + profiler setup
# ------------------------------
python_cmd=("python3")
if (( profile == 1 )); then
    if [[ $profiler == "scorep" ]]; then

        # Profiling with scoreP does not work as not build on Vega with Python support
        # and not able to insall from scratch and make it work.
        echo "Profiler: $profiler is not supported!" >&2
        exit 1

        # # DO NOT LOAD SYSTEM SCORE-P
        # # module load Score-P/8.4-gompi-2024a    # REMOVE THIS

        # mkdir -p "${res_path}scorep/${ident}"

        # container_python="/opt/python314t/bin/python3"

        # # ScoreP environment
        # export SCOREP_HOME=/ceph/hpc/data/d2025d03-111-users/scorep-8.4 # scoreP installed with install-scorep-8.4.sh
        # export SINGULARITYENV_SCOREP_HOME=$SCOREP_HOME
        # export PATH=$SCOREP_HOME/bin:$PATH
        # export LD_LIBRARY_PATH=$SCOREP_HOME/lib:$LD_LIBRARY_PATH

        # export SINGULARITYENV_PATH=$PATH
        # export SINGULARITYENV_LD_LIBRARY_PATH=$LD_LIBRARY_PATH

        # if [[ $parallel_type == "mpi" ]]; then
        #     python_cmd=( "scorep" "--mpp=mpi" "$container_python")
        # else
        #     python_cmd=( "scorep" "$container_python")
        # fi
        # python_cmd=( "scorep" "--mpp=mpi" "$container_python")

    elif [[ $profiler == "viztracer" ]]; then
        mkdir -p "${res_path}viztracer"
        python_cmd=("python3" "-m" "viztracer" "--quiet" "-o" "${res_path}viztracer/${ident}.json")

    else
        echo "Unknown profiler: $profiler" >&2
        exit 1
    fi
fi

# ------------------------------
# Combine the full Singularity command
# ------------------------------
cmd+=("$sif" "${python_cmd[@]}" "${cmd_args[@]}")

# ------------------------------
# Debug print
# ------------------------------

# echo "=============== DEBUG ===============" >&2
# echo "SHELL                = $SHELL" >&2
# echo "BASH_VERSION         = $BASH_VERSION" >&2
# echo "PATH                 = $PATH" >&2
# echo "which singularity    = $(which singularity)" >&2
# echo "sif                  = $sif" >&2
# echo "ls -lh sif:" >&2
# ls -lh "$sif" >&2 2>/dev/null || echo "SIF NOT FOUND" >&2

# echo "---- cmd array ----" >&2
# for i in "${!cmd[@]}"; do
#     printf 'cmd[%d] = <%s>\n' "$i" "${cmd[$i]}" >&2
# done

# echo "---- full srun command assembled ----" >&2
# echo srun ${mpi_flag} --nodes="${nodes}" --ntasks-per-node="${ranks_per_node}" --cpus-per-task="${cpus_per_task}" "${cmd[@]}" >&2
# echo "=============== END DEBUG ===============" >&2

# echo "=== ENV inside job before srun ==="
# env | grep -E "SCOREP|PATH|LD_LIBRARY_PATH|MPI" >&2
# echo "=================================="

# ------------------------------
# Execute via SLURM
# ------------------------------

srun \
    "${mpi_flag[@]}" \
    --nodes="${nodes}" \
    --ntasks-per-node="${ranks_per_node}" \
    --cpus-per-task="${cpus_per_task}" \
    "${cmd[@]}" \
    > "${res_path}${ident}.txt"
